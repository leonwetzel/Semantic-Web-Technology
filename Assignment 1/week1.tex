\documentclass[a4paper, 11pt]{article}

\usepackage[a4paper, total={6.5in, 8.5in}]{geometry}
\usepackage[round]{natbib}

\usepackage[hyphens]{url}
\usepackage[hidelinks]{hyperref}

\usepackage[parfill]{parskip}
\usepackage{dirtytalk}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage{float}

%opening
\title{Semantic Web Technology -- Assignment 1}
\author{Leon F.A. Wetzel\\s3284174\\ \texttt{l.f.a.wetzel@student.rug.nl}}

\begin{document}

\maketitle

\section{Introduction}

In this report, we take a closer look at two systems designed for the task of named entity linking. We use different English corpora to evaluate the performance of both systems and to determine if they perform well or not. Altough there are a number of systems capable of named entity linking, we only take a look at \textbf{TextRazor} and \textbf{OpenTapioca} as this fits in the scope of the actual assignment. As describing both models does not fall in the scope of this assignment, we refer to the documentation of TextRazor \citep{crayston_2020} and OpenTapioca \citep{DBLP:journals/corr/abs-1904-09131} for more information about their respective history and architecture.

This report covers several aspects of these systems. We take a look at the data which we will use for the evaluation, we discover more about the metrics used for the evaluation and we take an in-depth look at the actual annotations that were generated by these systems.

\section{Data}

Our test data for evaluating the entity linking systems is fairly simple. We use two English news articles which will be annotated by both systems independently. The first news article is a sample from \citet{mcgee_2020}, which covers the situation in Britain regarding new, controversial legislation for Brexit. The second news article is a sample from \citet{mckeever_2020} and explains more about upcoming COVID-19 vaccins. Both articles have been picked based on presence of named entities and differing topic.

Let's take a look at the corpora. Our sample from \citet{mcgee_2020} contains the following text:

\say{
	Ahead of a very crucial round of talks between London and Brussels over the future trading relationship between the UK and the European Union, the British government made a startling admission: That it would be prepared to break the terms of an international treaty.
	The threat was relatively technical -- over an aspect of the withdrawal agreement that allowed the UK to leave the EU at the end of January -- but the admission by a government minister in the House of Commons sent shockwaves through diplomatic circles and raised questions about whether the UK can be trusted on the world stage.
}

We can already spot several named entities by hand. The sample from \citet{mckeever_2020} contains the following text:

\say{
	More than 150 coronavirus vaccines are in development across the world—and hopes are high to bring one to market in record time to ease the global crisis. Several efforts are underway to help make that possible, including the U.S. government’s Operation Warp Speed initiative, which has pledged \$10 billion and aims to develop and deliver 300 million doses of a safe, effective coronavirus vaccine by January 2021. The World Health Organization is also coordinating global efforts to develop a vaccine, with an eye toward delivering two billion doses by the end of 2021. The candidates, like all vaccines, essentially aim to instruct the immune system to mount a defense, which is sometimes stronger than what would be provided through natural infection and comes with fewer health consequences.
}

A similar observation counts for \citet{mckeever_2020}: we can already spot a number of named entities. We can now feed the articles to both TextRazor and OpenTapioca.

\section{Using and evaluating the models}

Our way of working comprises of manually entering the samples from \citet{mcgee_2020} and \citet{mckeever_2020} in both TextRazor and OpenTapioca. We can then compare the output from both systems. We measure model performances by taking a look at precision (equation \ref{eq:precision}) and recall (equation \ref{eq:recall}), which take as input observed entities and real entities. Observed entities are entities that have been detected by a system, real entities are entities that are real-world entities. 

\begin{multicols}{2}
	\begin{equation}
	precision = \frac{TP}{TP + FP}
	\label{eq:precision}
	\end{equation}\break
	\begin{equation}
	recall = \frac{TP}{TP + FN}
	\label{eq:recall}
	\end{equation}
\end{multicols}

\subsection{TextRazor}

We start our journey with TextRazor. We head to the web interface by visiting \url{www.textrazor.com/demo}. A demonstration sample has already been given, but we will use our own corpora instead. We enter our samples from \citet{mcgee_2020} and \citet{mckeever_2020} and TextRazor returns us the annotations as shown in figures \ref{fig:doc1tr} and \ref{fig:doc2tr}.

\begin{figure}[H]
	\label{fig:doc1tr}
	\centering
	\includegraphics[width=0.9\textwidth]{doc1_textrazor.png}
	\caption{Generated annotations by TextRazor for \citet{mcgee_2020}.}
\end{figure}

We can see that - next to annotating the corpus - TextRazor splits its output per sentence, presumably for sake of readability. Our first observation is that TextRazor has performed  decently in underlining what relevant named entities are. We displayed the results of the performance of TextRazor in tables \ref{table:rep_doc1_tr} and \ref{table:rep_doc2_tr}. We can observe that TextRazor has found all real entities in the sample of \citet{mcgee_2020}, but that it was rather generous in observing entities as well.

\begin{figure}[H]
	\label{fig:doc2tr}
	\centering
	\includegraphics[width=0.9\textwidth]{doc2_textrazor.png}
	\caption{Generated annotations by TextRazor for \citet{mckeever_2020}.}
\end{figure}


\subsection{OpenTapioca}

The web interface of OpenTapioca (\url{www.opentapioca.org}) has a very minimalistic and simple design when compared to TextRazor, When it comes to functionality, they are nearly identical. Once again, a demonstration sample is already provided and we use our own samples instead.

\begin{figure}[H]
	\label{fig:doc1ot}
	\centering
	\includegraphics[width=0.7\textwidth]{doc1_ot.png}
	\caption{Generated annotations by OpenTapioca for \citet{mcgee_2020}.}
\end{figure}

\begin{figure}[H]
	\label{fig:doc2ot}
	\centering
	\includegraphics[width=0.7\textwidth]{doc2_ot.png}
	\caption{Generated annotations by OpenTapioca for \citet{mckeever_2020}.}
\end{figure}

The output (see figures \ref{fig:doc1ot} and \ref{fig:doc2ot}) is displayed on the same page, and named entities are \textit{highlighted} by using brackets. A quick observation shows that OpenTapioca makes more directly visible mistakes when compared to TextRazor. 

\subsection{Overview}

All results of our tests can be found in table \ref{tab:results}. When comparing the models using their precision and recall values, we can accept that TextRazor outperforms OpenTapioca. When we look at the generated annotations by both TextRazor and OpenTapioca, we can see that they behave very differently from each other. For example, we can see that TextRazor is eager to also link numbers used to describe amounts of money. Although this does not have to be marked wrong, it shows that the linking goes beyond the classical tagging of named entities.

\begin{table}[h]
	\centering
	\begin{tabular}{llll}
	\textbf{Corpus}	& \textbf{Model} & \textbf{Precision} & \textbf{Recall} \\ \hline
	\citet{mcgee_2020}	& TextRazor & 0.7273 & 1.0  \\
	\citet{mcgee_2020}	& OpenTapioca & 0.2857 &  0.5714 \\
	\citet{mckeever_2020}	& TextRazor & 0.75 & 0.5714    \\
	\citet{mckeever_2020}	& OpenTapioca & 0.1765 & 0.375  \\
	\end{tabular}
	\caption{Test results for TextRazor and OpenTapioca when applied to the \citet{mcgee_2020} and \citet{mckeever_2020} samples.}
	\label{tab:results}
\end{table}

The performances of OpenTapioca are disappointing when compared to TextRazor. The system sometimes looks confused when tagging named entities. OpenTapioca also seems to be too eager when tagging certain words, such as \textit{Ahead} or \textit{can}. The cause for this phenomenon can be found in that these words could possibly be synonyms for \textit{real} entities; \textit{can} appears to lead to the entity Canada according to OpenTapioca. The system extensively uses Wikidata for the entity linking, hence why the tagging is rather \textit{aggressive} and strange when compared to TextRazor.

\bibliographystyle{plainnat}
\bibliography{refs.bib}

\newpage

\appendix

\section{Results of TextRazor}

\begin{table}[h!]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{lllll}
			\textbf{Entity}	& \textbf{Observed entity?} & \textbf{Real entity?} & \textbf{Normalized entity} \\ \hline
			London	& Yes & Yes & London \\
			Brussels	& Yes & Yes & Brussels \\
			UK & Yes & Yes & United Kingdom\\
			the European Union & Yes & Yes & European Union\\
			the British Government & Yes & Yes & Government of the United Kingdom\\
			international treaty & Yes & No & Treaty\\
			Withdrawal agreement & Yes & No & Brexit withdrawal agreement\\
			the UK to leave the EU & Yes & No & Brexit\\
			January & Yes & Yes & 2021-01-01T00:00:00.000+00:00\\
			government minister & Yes & Yes & Minister (government)\\
			House of Commons & Yes & Yes & House of Commons
		\end{tabular}%
	}
	\caption{Performance report of TextRazor analyzing the \citet{mcgee_2020} sample.}
	\label{table:rep_doc1_tr}
\end{table}

\begin{table}[h!]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{lllll}
			\textbf{Entity}	& \textbf{Observed entity?} & \textbf{Real entity?} & \textbf{Normalized entity} \\ \hline
			150	& Yes & No & 150 \\
			coronavirus vaccines	& Yes & Yes & Coronavirus vaccine \\
			world & No & Yes & N.A.\\
			crisis & No & Yes & N.A.\\
			U.S. & Yes & No & United States\\
			U.S. government & No & Yes & N.A.\\
			Operation Warp Speed & Yes & Yes & Operation Warp Speed\\
			\$10 billion & Yes & No & 10000000000\\
			300 million & Yes & No & 300000000\\
			2021 & Yes & Yes & 2021\\
			The World Health Organization & Yes & Yes & World Health Organization\\
			vaccine & Yes & Yes & Vaccine\\
			eye & Yes & Yes & Eye\\
			two billion & Yes & No & 2000000000\\
			immune system & Yes & Yes & Immune system\\
			infection & Yes & Yes & Infection\\
			health & Yes & Yes & Health
		\end{tabular}%
	}
	\caption{Performance report of TextRazor analyzing the \citet{mckeever_2020} sample.}
	\label{table:rep_doc2_tr}
\end{table}

\end{document}
